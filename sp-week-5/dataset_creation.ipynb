{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf91e840-8128-4e25-baf8-40a05492f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdp_data import CDPInstances, datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4bc34b-49cf-42fc-a23f-8ff2378e170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation_dataset(city, output_file_name):\n",
    "    # 'city' should be one of seattle, louisville, oakland, or alameda\n",
    "    \n",
    "    if city == 'seattle':\n",
    "        city_transcripts = datasets.get_session_dataset(\n",
    "            CDPInstances.Seattle,  # specify the city (or county) council we want data from\n",
    "            start_datetime=\"2020-01-01\",  # YYYY-MM-DD format\n",
    "            end_datetime=\"2023-04-01\",  # YYYY-MM-DD format\n",
    "            store_transcript=True,  # store transcripts locally for fast file reading\n",
    "            store_transcript_as_csv=True,  # store transcripts as CSVs for easy pandas reading\n",
    "            raise_on_error=False\n",
    "        )\n",
    "    elif city == 'louisville':\n",
    "        city_transcripts = datasets.get_session_dataset(\n",
    "            CDPInstances.Louisville,\n",
    "            start_datetime=\"2020-01-01\",\n",
    "            end_datetime=\"2023-04-01\",\n",
    "            store_transcript=True,\n",
    "            store_transcript_as_csv=True,\n",
    "            raise_on_error=False\n",
    "        )\n",
    "    elif city == 'oakland':\n",
    "        city_transcripts = datasets.get_session_dataset(\n",
    "            CDPInstances.Oakland,  \n",
    "            start_datetime=\"2020-01-01\",\n",
    "            end_datetime=\"2023-04-01\",\n",
    "            store_transcript=True,\n",
    "            store_transcript_as_csv=True,\n",
    "            raise_on_error=False\n",
    "        )\n",
    "    elif city == 'alameda':\n",
    "        city_transcripts = datasets.get_session_dataset(\n",
    "            CDPInstances.Alameda,  \n",
    "            start_datetime=\"2020-01-01\",  \n",
    "            end_datetime=\"2023-04-01\",  \n",
    "            store_transcript=True,  \n",
    "            store_transcript_as_csv=True,  \n",
    "            raise_on_error=False\n",
    "        )\n",
    "\n",
    "    city_transcripts['muni'] = city\n",
    "\n",
    "    # Randomly sample 50 events from all of the events from each council \n",
    "    random_50 = city_transcripts.sample(n=50)\n",
    "\n",
    "    # get all sentences\n",
    "    city_sentences_session_df = {}\n",
    "\n",
    "    for i, session in random_50.iterrows():\n",
    "    \n",
    "        sentence_df = pd.read_csv(session.transcript_as_csv_path)\n",
    "    \n",
    "        # to keep session_id information associated with each sentence, \n",
    "        # create another dataframe with session_id column and muni column\n",
    "        city_sentences_session_df[session.transcript_as_csv_path] = sentence_df.assign(session_id = session.id, muni = session.muni)\n",
    "    \n",
    "\n",
    "    city_sentences_session_df = pd.concat(city_sentences_session_df).reset_index()\n",
    "    city_sentences_session_df = city_sentences_session_df.drop(columns=['level_0', 'level_1'])\n",
    "    # city_sentences_session_df has all sentence data for all 50 events (with session id)\n",
    "\n",
    "\n",
    "    # Create chunks of 5 sentences at a time. \n",
    "\n",
    "    # group by session_id\n",
    "    grouped = city_sentences_session_df.groupby('session_id')\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for session_id, group in grouped:\n",
    "        for i in range(0, len(group), 5):\n",
    "            chunk = group.iloc[i:i+5]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "\n",
    "    cleaned_chunks = []\n",
    "\n",
    "    for df in chunks:\n",
    "        df = df[['text', 'session_id', 'muni']]\n",
    "        df = df.reset_index(drop=True)\n",
    "        cleaned_chunks.append(df)\n",
    "\n",
    "\n",
    "    chunks_list = []\n",
    "    session_id_list = []\n",
    "    muni_list = []\n",
    "\n",
    "    for chunk in cleaned_chunks:\n",
    "        text = ''.join(str(chunk['text'].tolist())).replace('[','').replace(']','').replace(\"'\",'').replace(',', ' ').replace('\"', '')\n",
    "        chunks_list.append(text)\n",
    "        session_id_list.append(chunk['session_id'][0])\n",
    "        muni_list.append(chunk['muni'][0])\n",
    "\n",
    "    chunks_df = pd.DataFrame({'text': chunks_list, 'session_id': session_id_list, 'muni': muni_list})\n",
    "\n",
    "\n",
    "    # Randomly sample 1000 of the chunks\n",
    "\n",
    "    random_1000_from_chunks_df = chunks_df.sample(n=1000).reset_index()\n",
    "\n",
    "\n",
    "    random_1000_df = random_1000_from_chunks_df.drop(columns=['index'])\n",
    "\n",
    "    # add a new column meta\n",
    "    random_1000_df['meta'] = random_1000_df.apply(lambda row: {'muni': row['muni'], 'session_id': row['session_id']}, axis=1)\n",
    "\n",
    "\n",
    "    cleaned_random_1000_df = random_1000_df.drop(columns=['muni', 'session_id'])\n",
    "    \n",
    "    # export\n",
    "    cleaned_random_1000_df.to_json(output_file_name, orient='records', lines=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e34634-162c-43d3-a2ad-0dbd607ca02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76196c39893c49f9a99b2a4c86f83fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching each model attached to event_ref:   0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a359ad8066a34034b009a268045f1796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcripts:   0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c02e26bc4149eca019d9cdc77e76b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting transcripts:   0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ff67079439498299aeffc9ecb836cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching each model attached to event_ref:   0%|          | 0/1059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d927f288e0e44bc887d2da43d3bdc417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcripts:   0%|          | 0/1059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kellyzwang/miniconda3/envs/sig-cdp/lib/python3.9/site-packages/cdp_data/datasets.py:774: TqdmWarning: Iterable length 1059 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  converted_transcript_infos = process_map(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021b55c59ecf4012a49e1b410048d1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting transcripts:   0%|          | 0/1059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f2a51eed8b4fb1886ff4e08c1a0632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching each model attached to event_ref:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c908d5955d674a7c88fdcc5ac1ed7ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcripts:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9a1be639a44ea6a30ee5e4ac75f117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting transcripts:   0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa06358bfd8b4b939ef19be1e0853430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching each model attached to event_ref:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378601789c7e4a4288b130f6899c94db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcripts:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91f0b8b4db44481bf98e68b7fe3cb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting transcripts:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_annotation_dataset('seattle', 'seattle_dataset.jsonl')\n",
    "create_annotation_dataset('louisville', 'louisville_dataset.jsonl')\n",
    "create_annotation_dataset('oakland', 'oakland_dataset.jsonl')\n",
    "create_annotation_dataset('alameda', 'alameda_dataset.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
