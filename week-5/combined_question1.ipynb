{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f9d978-26e5-470d-8eb2-530a1cbe13a9",
   "metadata": {},
   "source": [
    "# Pulling Data\n",
    "\n",
    "Use three months of data from Seattle, Oakland, and Louisville\n",
    "- start date of \"2022-09-01\"\n",
    "- end date of \"2022-12-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4656a55c-ff64-4702-9517-80e3113204f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdp_data import CDPInstances, datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c546ad-8be8-4aa3-8f8c-77baa6cf0ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03897d5d46243708bd469ef06bf74d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching each model attached to event_ref:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01732cae4dfe4a7b93b7c92dd5c71868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcripts:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting and storing each transcript as a CSV: 17it [00:08,  1.98it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138379fac696411dbe26bba753a2c3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching each model attached to event_ref:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d457eb511cfd41669c342f46816ceaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcripts:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting and storing each transcript as a CSV: 18it [00:09,  1.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdcae93136c420ab5a219945b50bfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching each model attached to event_ref:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9027e1224d7144de87b1cf0a8ac9e609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching transcripts:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting and storing each transcript as a CSV: 77it [00:32,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get a dataset of \"city council sessions\" for Seattle 2022-09-01 to 2022-12-01\n",
    "seattle_transcripts_sep_2022_to_dec_2022 = datasets.get_session_dataset(\n",
    "    CDPInstances.Seattle,  # specify the city (or county) council we want data from\n",
    "    start_datetime=\"2022-09-01\",  # YYYY-MM-DD format\n",
    "    end_datetime=\"2022-12-01\",  # YYYY-MM-DD format\n",
    "    store_transcript=True,  # store transcripts locally for fast file reading\n",
    "    store_transcript_as_csv=True,  # store transcripts as CSVs for easy pandas reading\n",
    ")\n",
    "\n",
    "# Get a dataset of \"city council sessions\" for Oakland 2022-09-01 to 2022-12-01\n",
    "oakland_transcripts_sep_2022_to_dec_2022 = datasets.get_session_dataset(\n",
    "    CDPInstances.Oakland,  # specify the city (or county) council we want data from\n",
    "    start_datetime=\"2022-09-01\",  # YYYY-MM-DD format\n",
    "    end_datetime=\"2022-12-01\",  # YYYY-MM-DD format\n",
    "    store_transcript=True,  # store transcripts locally for fast file reading\n",
    "    store_transcript_as_csv=True,  # store transcripts as CSVs for easy pandas reading\n",
    ")\n",
    "\n",
    "# Get a dataset of \"city council sessions\" for Louisville 2022-09-01 to 2022-12-01\n",
    "louisville_transcripts_sep_2022_to_dec_2022 = datasets.get_session_dataset(\n",
    "    CDPInstances.Louisville,  # specify the city (or county) council we want data from\n",
    "    start_datetime=\"2022-09-01\",  # YYYY-MM-DD format\n",
    "    end_datetime=\"2022-12-01\",  # YYYY-MM-DD format\n",
    "    store_transcript=True,  # store transcripts locally for fast file reading\n",
    "    store_transcript_as_csv=True,  # store transcripts as CSVs for easy pandas reading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b5601-b713-4126-ae5e-6ec974e06cc2",
   "metadata": {},
   "source": [
    "# Recognizing NERs using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65e4cc4-8356-48df-a8c4-19f0a90f952d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# The NERs we are interested in\n",
    "ners = ['PERSON', 'ORG', 'NORP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3c380e-e151-4533-9af2-f024fa81772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seattle\n",
    "seattle_rows = []\n",
    "seattle_sentences_dfs = {}\n",
    "\n",
    "for i, session in seattle_transcripts_sep_2022_to_dec_2022.iterrows():\n",
    "    \n",
    "    seattle_sentences_df = pd.read_csv(session.transcript_as_csv_path)\n",
    "    # drop rows if text column is empty\n",
    "    seattle_sentences_df = seattle_sentences_df.dropna(subset=[\"text\"])\n",
    "    \n",
    "    # to keep session_id\n",
    "    seattle_sentences_dfs[session.transcript_as_csv_path] = seattle_sentences_df.assign(session_id = session.id)\n",
    "    \n",
    "    for t in seattle_sentences_df.index:\n",
    "        doc = nlp_spacy(str(seattle_sentences_df.text[t]))\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ners:\n",
    "                seattle_rows.append([session.id, t, ent.label_, \n",
    "                                     ent.text, 'Seattle'])\n",
    "    \n",
    "            \n",
    "seattle_df = pd.DataFrame(seattle_rows, columns=[\"session_id\", \"sentence_index\", \"named_entity_type\", \n",
    "                                                 \"entity\", \"city\"])\n",
    "\n",
    "seattle_all_sentence_df = pd.concat(seattle_sentences_dfs).reset_index()\n",
    "\n",
    "\n",
    "# oakland\n",
    "oakland_rows = []\n",
    "oakland_sentences_dfs = {}\n",
    "\n",
    "for i, session in oakland_transcripts_sep_2022_to_dec_2022.iterrows():\n",
    "    \n",
    "    oakland_sentences_df = pd.read_csv(session.transcript_as_csv_path)\n",
    "    # drop rows if text column is empty\n",
    "    oakland_sentences_df = oakland_sentences_df.dropna(subset=[\"text\"])\n",
    "\n",
    "    # to keep session_id\n",
    "    oakland_sentences_dfs[session.transcript_as_csv_path] = oakland_sentences_df.assign(session_id = session.id)\n",
    "        \n",
    "    for t in oakland_sentences_df.index:\n",
    "        doc = nlp_spacy(str(oakland_sentences_df.text[t]))\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ners:\n",
    "                oakland_rows.append([session.id, t, ent.label_, \n",
    "                                     ent.text, 'Oakland'])\n",
    "\n",
    "\n",
    "oakland_df = pd.DataFrame(oakland_rows, columns=[\"session_id\", \"sentence_index\", \"named_entity_type\", \n",
    "                                                 \"entity\", \"city\"])\n",
    "\n",
    "\n",
    "oakland_all_sentence_df = pd.concat(oakland_sentences_dfs).reset_index()\n",
    "\n",
    "\n",
    "# louisville\n",
    "louisville_rows = []\n",
    "louisville_sentences_dfs = {}\n",
    "\n",
    "for i, session in louisville_transcripts_sep_2022_to_dec_2022.iterrows():\n",
    "    \n",
    "    louisville_sentences_df = pd.read_csv(session.transcript_as_csv_path)\n",
    "    # drop rows if text column is empty\n",
    "    louisville_sentences_df = louisville_sentences_df.dropna(subset=[\"text\"])\n",
    "    \n",
    "    # to keep session_id\n",
    "    louisville_sentences_dfs[session.transcript_as_csv_path] = louisville_sentences_df.assign(session_id = session.id)\n",
    "\n",
    "    for t in louisville_sentences_df.index:\n",
    "        doc = nlp_spacy(str(louisville_sentences_df.text[t]))\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ners:\n",
    "                louisville_rows.append([session.id, t, ent.label_, \n",
    "                                        ent.text, 'Louisville'])\n",
    "\n",
    "\n",
    "louisville_df = pd.DataFrame(louisville_rows, columns=[\"session_id\", \"sentence_index\", \"named_entity_type\", \n",
    "                                                       \"entity\", \"city\"])\n",
    "\n",
    "louisville_all_sentence_df = pd.concat(louisville_sentences_dfs).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cc203-9df8-43fc-b302-4b479de9e9c4",
   "metadata": {},
   "source": [
    "# Combining all NER results \n",
    "\n",
    "- Combined `seattle_df`, `oakland_df`, and `louisville_df` into 1 dataframe `all_cities_df`.\n",
    "\n",
    "- Combined `seattle_all_sentence_df`, `oakland_all_sentence_df`, and `louisville_all_sentence_df` into 1 dataframe `all_cities_sentences_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef6e3d3-a442-4934-adab-d2fb105fcc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities_df = pd.concat([seattle_df, oakland_df, louisville_df])\n",
    "\n",
    "all_cities_sentences_df = pd.concat([seattle_all_sentence_df, \n",
    "                                     oakland_all_sentence_df, \n",
    "                                     louisville_all_sentence_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c833076-06bb-4cde-94fc-ca82f04dda79",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "- Divide index by total number of sentences == % of meeting where sentence appears\n",
    "\n",
    "- Add a new column to all_cities_df 'percentages' indicating when an entity is recognized in the meeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27095664-1c37-4b4d-bc01-c35aa08aff5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
